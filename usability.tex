There are different interpretations on what usability and user experience mean. It is important to understand these words so that a clearer idea can be formed on what needs to be evaluated. This section will give an overview of the state of the art regarding usability and user experience, and how they can be measured.

\section{Meaning of usability and user experience} \label{definitions_section}
The definitions of usability and user experience in the context of software evaluation aren't self-evident, and they can be interpreted in many different ways. However, these terms do have official ISO standards defining them. Bevan et al.talk about the importance of using these standards so that the criteria against which software is evaluated stays consistent. \cite{bevanstandard}

In  ISO FDIS 9241-210, usability is defined as "Extent to which  a system, product or service can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use." Therefore, usability seems to have its main focus on the pragmatic goal of achieving some task in a efficient way while using the system, however it also includes the concept of satisfaction.

User experience on the other hand is defined as "A person's perceptions and responses that result from the use and/or anticipated use of a product, system or service." It would appear that user experience is more about deriving pleasure from using a system even if it is not for entertainment purposes. Also worth noting is that this also covers anticipated use, before the user has even seen the system, and how their expectations may compare to reality.

There are many ways to interpret how these terms relate to each other. If the concept of "satisfaction" in the definition of usability is considered to cover "a person's perceptions and responses" in the definition of user experience, then usability can be seen as a umbrella term that covers both the actual measurable work efficiency and personal feelings that stem from using the system.

Väänänen-Vainio-Mattila et al. and Bevan note that in industry it is often user experience that is used as an umbrella term that includes the work efficiency component of usability, while in research community user experience is seen as the subjective perception that user gets from using a system. Because of its subjective nature, evaluation methods for this this academic definition of user experience aren't well-established yet. \cite{bevan2009difference, vaananen2008towards}

In the end there are three different interpretations of user experience according to Bevan: \begin{itemize}
\item An  elaboration  of  the  satisfaction  component  of usability
\item Distinct  from  usability,  which  has  a  historical emphasis on user performance
\item An  umbrella  term  for  all  the  user’s  perceptions  and responses,  whether  measured  subjectively  or objectively
\end{itemize}
\cite{bevan2009difference}

Petrie and Bevan further open up the meanings of these terms and their components and how they're used in research community. Usability can be contain concepts like learnability, flexibility, memorability, safety and accessibility in addition to efficiency, because all these contribute to achieving some end goal through the use of the system. What exactly is considered usability is dependent to the system in question. 

According to Petrie and Bevan, users may want more from their interaction with the system than just to complete a task efficiently, and this is where user experience is an important thing to consider. Information technology has become an ubiquitous part of everyday life and is no longer just a means to an end to people.
\cite{bevanevaluation}

\cite{bevan2009difference, bevaniso, bevanevaluation, bevanstandard}

Tullis and Albert consider user experience to be a broad term that covers the user's entire interaction with the system, including their thoughts, feelings and perceptions. Their view of user experience metrics is that they reveal information about the effectiveness, efficiency and satisfaction that's a result of the interaction between the user and the system. \cite{albert2013measuring}

Hassenzahl et al. look at usability and user experience from different points of view and argue that while these terms practically cover the same things, they have slightly different focuses. Usability is more concerned with practical task completion and objectively measurable metrics, while user experience tries to balance the practical and hedonic sides of information system usage and also considers how the user feels after using the system.
\cite{hassenzahl2006user}

In Rubin's and Chisnell's definition, with an usable system "the user can do what he or she wants to do the way he or she expects to be able to do it, without hindrance, hesitation, or questions". An usable system should be "useful, efficient, effective, satisfying, learnable, and accessible". \cite{rubin2008handbook} This way of defining usability is similar to others and also covers the concept of user satisfaction. An user is more likely to use a system that generates positive feelings.

In all different definitions, usability seems to cover the basic idea that an user should be able to complete a task using the system efficiently. What is less clear is whether user experience is a part of usability or vice versa, or whether they are two separate concepts. There is support for many different interpretations.

In this thesis, user experience will be used to describe the user's perceived feelings regarding the system. Usability is going to be used to define the more pragmatic task-oriented part of using the system.

% Usability Models

\section{Usability modeling}\label{usability_attributes}
Usability models split the concept of usability into different components which can be linked to some measurable values. Many different concepts like efficiency and learnability were already mentioned in the section \ref{definitions_section}.

Abran et al. propose an usability model that enhances upon earlier models and is based on ISO standards. This model includes effectiveness, efficiency, satisfaction, security and learnability. \cite{abran2003usability} Security is the one term that they added on top of the ISO definition of usability.

There isn't one standard usability model, but many of them build on the ISO definition of usability and cover the same basic ideas. At least the following components can be found across different models and definitions of usability:

\textbf{Usefulness} is a measure of whether the system is actually needed. The task the system is designed for should be something that people need to or want to do, and the system should be helpful in completing that task. One way to measure this would be to ask users to complete a task without a system and then with it, and compare the results.

\textbf{Efficiency} is a measure of how good are the results of using the system relative to the resources it needs. This is often measured in the amount of time required to complete a task, which is possible to evaluate as long as the task is well defined and has a starting and an ending time. Other things like the amount of physical and mental effort are also linked to efficiency.

\textbf{Effectiveness} is defined as how well the users can complete tasks using the system. The system should act reliably in a way that the user expects it to in order to be properly usable. Otherwise, the tasks may end up with erroneous results or not completed at all. Comparing the amount of succesfully completed tasks to the amount of failed tasks is one way to measure effectiveness.

\textbf{Learnability} means how quickly and how well the users are able to learn the system so that they can use it effectively. If the system is hard to learn, this will lead to problems in effectiveness and efficiency. Having people use a system for some time and seeing how long it takes for them to use the system effectively can be used as a measure of learnability.

\textbf{Satisfaction} is a hedonic measure that covers the user's feelings about using the system. It is useful to directly ask the users about their opinions and preferences as these can have an effect on perceived usability. At very least, the user shouldn't feel negative emotions like frustration or confusion while and after using the system. Optimally the system would evoke some positive reaction that could encourage the user to interact with the system more. If the system is learnable, efficient and effective, it should be satisfactory to use, but also things like the visual look of the system can have an effect. A questionnaire at the end of evaluation and also asking questions and observing the participant's positive and negative reactions during the evaluation are some ways of collecting data about user satisfaction. \cite{abran2003usability, winter2008comprehensive}

\textbf{Accessibility} often deals with how well the system can be used by people with disabilities, for example poor eyesight or coordination.

\textbf{Memorability} is  a subset of learnability; how well and how quickly the user can continue using the system after being away from it for some time.

\textbf{Flexibility} means the capability of the system to accommodate to changes desired by the user.

\textbf{Safety} covers the aspects of the system protecting the user from dangerous situations and undesirable conditions. \cite{bevanevaluation, winter2008comprehensive}

\textbf{Security} could be considered to overlap with safety; the system should be able to prevent unauthorized access to its functionality and data. \cite{abran2003usability}

\cite{albert2013measuring, rubin2008handbook}

\section{Evaluating usability and user experience}
In this section the available methods of evaluation will be overviewed to get a better understanding on what data should be collected in an evaluation, and what are particularly important things to take into account to ensure the validity of the data.

\subsection{Evaluation methods}
According to Rubin and Chisnell, an usability evaluation has the following basic elements:
\begin{itemize}
\item Develop research questions
\item Use a representative sample of end users
\item Represent the actual work environment
\item Observe the end users using the product
\item Interview the participants
\item Collect quantitative and qualitative performance and preference measures
\item Recommend improvements to design
\end{itemize}

\cite{rubin2008handbook}

While using real users in an empirical evaluation is time-consuming, is often the best way to identify usability problems. During the development cycle, using usability inspection methods like heuristic evaluation is sometimes used, which involves an expert going through the interface and comparing it against a set of criteria such as Nielsens ten heuristics \cite{Solr-oula.410573, nielsen1995usability}. This is faster and easier than doing full usability evaluation with users, but doesn't identify as many problems.

Hollingsed and Novick note that using both empirical evaluation and usability inspection would be the most effective solution that is often overlooked. \cite{hollingsed2007usability}

An usability inspection such as heuristic evaluation can be performed by doing a walkthrough of the system from a developer's perspective.

User evaluations can be done by collecting data during day-to-day usage, for example by showing a survey to the user at random times, or by inspecting from logs how the users interact with the system. \cite{bevanevaluation}

Data can be collected concurrently during the test or after it retrospectively. Van den Haak et al. performed a comparative test between concurrent think-aloud (CTA) and retrospective think-aloud (RTA).

CTA was found to find more task-oriented errors while RTA was good for gaining broader user reactions. There is some debate on whether or not CTA has a negative effect on task performance because talking aloud would distract the user, but some have found it to have a positive effect. When performing a complex task the user may verbalize less. RTA is less susceptible this problem. Task complexity for evaluation should be designed so that the users are required to think, but aren't over-burdened so much that they cannot make verbal observations.
\cite{van2003retrospective}

\subsection{Participant selection}
Selecting the right participants for the study is a very important aspect of user experience evaluation. The participants should be a representative sample of the target userbase of the system. If the test users are selected incorrectly the test results would not be a useful metric of the user experience in real use.

\cite{rubin2008handbook, albert2013measuring}

\subsection{Tasks}
The participants should perform different tasks during an evaluation. These tasks should represent some real use case of the system that is intended to be evaluated, and they should be able to bring out differences in usability between the evaluated system. The tasks should require the users to make use of different functionalities of the system.

If the amount of participants is rather small, every user should perform the same set of tasks, as it may not viable to have a different set of users for each task. This means that the evaluation would use a within-subjects design rather than a between-subjects design. 

One thing that is worth noticing is that when users perform tasks, they gradually learn to use the system. This is why users should perform these tasks in different orders to mitigate the learning effect. This distribution of task order is called counter-balancing. The order in which different systems are shown to the participant can also be alternated.
\cite{rubin2008handbook}

\subsection{Metrics}
An evaluation should try to answer the questions defined in section \ref{research_questions}. To do this, data metrics need to be collected on different components of usability, which were defined in the section \ref{usability_attributes}.

For measuring pragmatic usability components like efficiency and effectiveness, common measures are task completion time and error rate. Sometimes a task may require multiple steps during which the user can make multiple errors. 

It is also possible to measure the input rate e.g. the amount of mouse clicks the user makes while performing a task.

Some tasks can have a pre-defined optimal solution that is as efficient as possible; users may not take this optimal path, so the possible extra steps and mistakes can be observed. \cite{hornbaek2006current}

Measuring hedonic user experience is commonly done by asking the participant a set of questions about their experience. Getting spontaneous and immediate reactions from the user at the end of the evaluation is an important part of gaining understanding about the user experience. The participants shouldn't be forced to analyze small details which they may not properly remember anymore at the end of the evaluation. \cite{laugwitz2008construction}

Hornbaek notes that there are many already validated questionnaires for measuring usability and user experience, but despite this, studies often create their own questionnaires in ad hoc basis. \cite{hornbaek2006current} Some common examples of questionnaires are AttrakDiff \cite{hassenzahl2003attrakdiff}, Questionnaire for User Interaction Satisfaction (QUIS) \cite{chin1988questionnaire}, System Usability Scale (SUS) \cite{brooke1996sus} and User Experience Questionnaire (UEQ) \cite{laugwitz2008construction}.

In a study by Tullis et al. that compares SUS, QUIS and a few other questionnaires, SUS was found to be the simplest and often most reliable one. UEQ was not in this comparison. \cite{tullis2004comparison}

The questions in SUS are as follows, all of them rated on a five-choice scale from strongly disagree to strongly agree:

\begin{enumerate}
\item I think that I would like to use this system frequently
\item I found the system unnecessarily complex
\item I thought the system was easy to use                      
\item I think that I would need the support of a technical person to be able to use this system
\item I found the various functions in this system were well integrated
\item I thought there was too much inconsistency in this system
\item I would imagine that most people would learn to use this system very quickly
\item I found the system very cumbersome to use
\item I felt very confident using the system
\item I needed to learn a lot of things before I could get going with this system 
\end{enumerate}

\cite{brooke1996sus}

UEQ is split five different scales altogether containing 26 items which apply particularly for hedonic user experience. The items are opposite adjectives on a scale from 1 to 7, and the order of the polarities is randomized. The scales are as follows:

\textbf{Attractiveness:} the user's overall impression of the system, for example "annoying - enjoyable".

\textbf{Perspiquity:} how easy to understand and clear the participants find the system to use, for example on a range "not understandable - understandable". This overlaps with learnability that was covered in section \ref{usability_attributes}

\textbf{Dependability:} how well the user can trust the system and predict what it does, for example on a scale "unpredictable - predictable". This is linked to how effectively the users can use the system.

\textbf{Efficiency:} this is the perceived efficiency of the system. Also considers how organized the interface is. Scale "impractical - practical" is one item of this.

\textbf{Novelty:} the novelty factor of the system has an effect on how eager people are to use it, and how much they will explore different options. This includes items like "creative - dull" and "conservative - innovative".

\textbf{Stimulation:} how engaged users feel when using the system. "Boring - exciting" is an example of this, as well as "motivating - demotivating".

Perspiquity, dependability and efficiency are task-oriented factors and should show a negative correlation with task completion time and error rate. Novelty and stimulation factors only show a weak correlation according to Laugwitz et al. \cite{laugwitz2008construction}

UEQ is particularly meant for measuring user experience, not only usability. It has been found to be fast and efficient to implement, the drawback being that the information is rather high level and doesn't go into specific details. However, by combining the questionnaire with a concrete usability evaluation it is possible to get more comprehensive results. \cite{schrepp2014applying, rauschenberger2013efficient}

Schrepp et al. have developed a benchmark for analyzing the results of UEQ. The benchmark was constructed based on 246 product evaluations to find baseline mean values and standard deviations for the different scales. This allowed the creation of different intervals on a scale from "bad" to "excellent". \cite{schrepp2017construction}

AttrakDiff is older than UEQ, and also meant for measuring perceived user experience, consisting of several different scales of user experience with seven-step items. The scales are "Attractiveness", "Pragmatic Quality", "Identity" and "Stimulation". These were found to have significant correlations with the scales of UEQ in a validation study. \cite{laugwitz2008construction} The original AttrakDiff is in German, it appears that the English version is only accessible through an online service. \cite{attrakdiff}

UEQ's item pool seemingly overlaps a lot with what is covered in SUS (complexity, consistency, confidence...), while presenting the questions in a simpler format, and also going into more detail than SUS. UEQ is easily available offline and has a convenient benchmark for analyzing the data.