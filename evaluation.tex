% Evaluation Chapter

This section covers the evaluation done on Cluster Web and OpsWeb. The aim of the evaluation is to gain knowledge about how the old and new version compare in terms of user experience and usability and to find possible usability problems. Research questions will be defined and the evaluation plan will be described in this chapter.

Because OpsWeb wasn't functionally yet on par with the old Cluster Web at this point of development as it was missing the pass planning functionality entirely, the evaluation focused on the data visualization functionality entirely; how well users can find information using the timeline interface.

There are some differences and new features in OpsWeb, and their effect on the user experience should be evaluated. In particular the configurable timelines are a new feature that could have a big effect on how people use Cluster Web, because it allows visualizing data from many different perspectives. This is arguably the largest addition compared to the old Cluster Web. Other features include the mouse-based timeline navigation, Uberlog integration on the timeline, and a moving live mode.

The results will hopefully shed some light on how much OpsWeb improves the user experience and opens up new possibilities with its new features. Having information about this could also help with setting goals and priorities for future development, but the study is more summative than formative as doing follow-up evaluations is outside of the scope of this thesis. \cite{albert2013measuring}

\section{Research questions} \label{research_questions}
The following main research question is the one that this evaluation aims to answer:

\textit{"How does the usability and user experience change between Cluster Web and OpsWeb?"}

This research question aims to cover all aspects of the evaluation, however there are a some more specific questions which can be made:

The configurability aspect of OpsWeb is an important part of the evaluation. In particular what could be interesting is how quickly people find data using different arrangements of timelines. This leads to the research question:

\textit{"How do different timeline visualization configurations affect usability?"}

Another aspect that can reasonably be expected to have a noticeable effect on usability is the ability to pan and zoom the timeline using the mouse. It should make user interaction more direct than clicking on buttons and waiting for the system to respond. This can be formulated into the following research question:

\textit{"How does direct mouse navigation affect the usability of a timeline interface compared to UI button-based navigation?"}

\section{Evaluation plan} \label{evaluation_plan}
The evaluation plan will be described in this section, including the task definitions and the questions which will be asked from the participants.

Background research regarding usability and user experience evaluation methods is covered in chapter \ref{evaluation_chapter}. User Experience Questionnaire (UEQ) \cite{laugwitz2008construction} was chosen as the method of gathering data on user experience, as it is openly available and has a statistical benchmark included. For measuring pragmatic usability, some metrics will be collected when users perform tasks. These include the task completion time, answer correctness and information about mouse usage, which should help in measuring the efficiency, effectiveness and other components of usability defined in section \ref{usability_attributes}. If participants make interesting observations they will be noted down.

\subsection{Course of the evaluation}
The evaluation is done one participant at a time in person, in their offices. All participants perform the evaluation on the same computer.

At the beginning of the evaluation, the participant is asked a set of background questions about their prior experience level with Cluster Web and OpsWeb. This is defined in section \ref{pre-evaluation}. Note that OpsWeb is called "New Cluster Web" in the evaluation questions.

The evaluation is split into two parts; Cluster Web and OpsWeb. The order in which these two systems are evaluated is alternated between participants to prevent the order from having an effect on the results. This will also have to be done when evaluation OpsWeb's different timeline configurations.

During the evaluation, participants will perform tasks which will require them to navigate the timeline and read information that is displayed on it. The tasks are defined in the section \ref{tasks_section}. The first task will be done with the first system and the other with the second system. The correct answers to the tasks were checked multiple times to ensure that the data had not changed.

The time to complete each task will be noted down from start to finish, and an application (Mousotron) \cite{mousotron} will be used to calculate the amount of mouse clicks and mouse movement distance. The given answer to the task will be noted down.

Default and Estrack configurations of OpsWeb will be comparatively evaluated to see how they affect the speed of finding information. The comparison tasks defined in the section \ref{tasks_section} will be done once the participants have seen all of the configurations. In addition, some configurations will not be comparatively evaluated, but they will be shown to the participant and explained, and after seeing all of the configurations the participants will rate their initial impression of each configurations' usefulness. The configurations are defined in the section \ref{configurations_section}.

 After evaluating a system the participants will fill out the UEQ based on their impressions of that system, and in the end of the evaluation they are asked for further feedback with the questionnaire defined in section \ref{post-evaluation}.

\subsection{Pre-evaluation questionnaire} \label{pre-evaluation}
Participants will answer these questions on a paper form before doing any of the tasks.
\begin{enumerate}
\item How experienced you are with the old Cluster Web? very inexperienced - very experienced (1-7)
\item How experienced you are with the new Cluster Web? very inexperienced - very experienced (1-7)
\item Do you agree with the following statement: "Cluster Web makes my job easier"? strongly disagree - strongly agree (1-7)
\item What are the main problems do you think the old Cluster Web has which could be improved with the new version?
\end{enumerate}

\subsection{Tasks} \label{tasks_section}
The tasks for the participants will be defined in this section.

\textbf{OpsWeb introduction}

If the participant hasn't used OpsWeb before, they should be introduced to the new features. Metrics won't be collected during this phase. The participants should be familiarized with the following features:
\begin{enumerate}
\item Timeline controls (pan, zoom, now button, calendar)
\item Live mode
\item Uberlog entries (meaning of icons, tooltips)
\item Countdown visualization on the timeline
\item Applying different configurations
\item Applying different style configurations (dark theme)
\item Sharing of the schedule timeframe with URL
\end{enumerate}

\textbf{Cluster Web - OpsWeb comparison tasks}

In the last 7 days of September this year, how many Maspalomas passes did...
\begin{enumerate}
\item Cluster 1 have? (Correct: 4)
\item Cluster 2 have? (Correct: 3)
\end{enumerate}

\textbf{OpsWeb configurations tasks}


In the last 7 days of September this year...
\begin{enumerate}
\item how many Yatharagga passes there were? (Correct: 3)
\item how many New Norcia passes there were? (Correct: 2)
\end{enumerate}

\subsection{Configurations} \label{configurations_section}
These configurations will be shown to the participants:

\textbf{Default:} this is the basic view that will be used for direct comparison between Cluster Web and OpsWeb. The schedule is split by spacecraft into four timelines, each of them displaying passes, SSR fill level, science modes, eclipses and apogees/perigees. In OpsWeb, Uberlog entries are also displayed on the timeline.

\textbf{Default, dark theme:} same as default configuration but in a dark color scheme.

\textbf{Estrack:} shows separate timelines for different ground stations instead of spacecrafts. Cluster passes can be displayed for each ground station. This allows looking at the data from a different perspective.

\textbf{Uberlog entries by severity level:} each spacecraft has its own timeline on which Uberlog entries are displayed. In addition, the spacecraft timelines are split into subtimelines by the Uberlog entries' severity level (confirmation - fatal) so that it can be seen quickly how many entries there are for each severity level. Passes can be displayed under the Uberlog entries to give them context.

\textbf{Uberlog entries by author:} each author of Uberlog entries is displayed on their own timelines, for example which spacecraft controller has been on shift can be seen in this configuration.

\textbf{LEOP:} In this "launch and early orbit phase" configuration, only information about a single spacecraft is displayed. Different types of data can have their own, dedicated timelines, such as scheduled passes, visibilities, SSR fill level, bookings by other missions. A countdown to the next pass can be displayed.

The question asked regarding each configuration will be as follows:
\begin{enumerate}
\item How useful do you find this configuration? not useful at all - very useful (1-7)
\end{enumerate}

\subsection{Post-evaluation questionnaire} \label{post-evaluation}
The following questions will be asked regarding OpsWeb's new features at the end of the evaluation:
\begin{enumerate}
\item How useful you find being able to pan and zoom the timeline using a mouse? not useful at all - very useful (1-7)
\item How useful you find being able to configure the timeline? not useful at all - very useful (1-7)
\item How useful you find displaying the Uberlog entries on the timeline? not useful at all - very useful (1-7)
\item How useful you find the live mode? not useful at all - very useful (1-7)
\item How useful you find the countdown on the timeline?
\item How useful you find being able to have different color themes?
\item How useful you find being able to share the schedule URL?
\item Do you have ideas how the new Cluster Web could be improved, or other comments?
\end{enumerate}

\section{Results}
The evaluation, as described in the section \ref{evaluation_plan}, was performed with eight participant. All of the participants were members of the Cluster II flight control team.

The evaluations were done over the course of two weeks, the latest build of OpsWeb from before the evaluations began was used. Evaluations were done in each participants' office or in the control room, using the author's work laptop with mouse and no external screen. Participants wrote down their answers on paper forms, which were then transferred into Excel sheets along with the author's observations. The evaluation took about half an hour per participant.

Overall the response towards OpsWeb was positive, although it is still in development and it will take some time for people to learn its features, especially configurations. Some potential usability issues were discovered during the evaluation which should be addressed in the future.

\subsection{Experience level of participants}
Participants reported their own prior experience level with both Cluster Web and OpsWeb, and if they found that Cluster Web makes their job easier. The answers on scale 1-7 are seen in table \ref{experience_levels}. It can be noted that everyone fully agreed that Cluster Web made their job easier, except for one participant who hadn't yet used Cluster Web enough to answer. Three of the participants were involved in OpsWeb development.

\begin{table}[!ht]
% Add some padding to the table cells:
\def\arraystretch{1.1}%
    \begin{center}
    \caption{Participants' answers to questions about their experience.}
    \label{experience_levels}
    \begin{tabular}{| l | l | l | l | l | l | l | l | l | }
    \hline
    Participant & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8   \\
    \hline
    CW experience    & 7  & 5 & 5 & 7 & 2 & 6 & 7 & 7     \\
    OW experience    & 1 & 5 & 6 & 7 & 1 & 2 & 2 & 1  \\
    CW makes job easier & 7 & 7 & 7 & 7 & - & 7 & 7 & 7    \\
    \hline
    \end{tabular}
    \end{center}
\end{table}

\subsection{Feedback about Cluster Web}
Participants mentioned some problems with Cluster Web they would like to be improved with OpsWeb. Slowness of navigating the timeline was by most participants. One participant mentioned that the menu system is very confusing to navigate for new users. The overall look and feel was said to be outdated by one user. For pass planning, some users would like to adjust pass times by dragging them with the mouse.

Another notable issue that some participants noted is the data processing workflow, which requires the mission planner to manually import files to the system every week. This is something that could be automated in the future. 

Lack of configurability and being mission specific were considered to be problems. Some users who were familiar with Cluster Web's code base mentioned that it is a very large, fragmented and consists of multiple different coding styles, needs better version control and is based on outdated technology (PHP). Applying it to other missions would be cumbersome.

\subsection{OpsWeb and Cluster Web task performance comparison} \label{cw_ow_task_perf}
The task measurements show some differences between Cluster Web and OpsWeb. Firstly, the task completion time average seems to be lower with OpsWeb, as seen in the table \ref{cw_ow_times}.

\begin{table}[!ht]
% Add some padding to the table cells:
\def\arraystretch{1.1}%
    \begin{center}
    \caption{Durations of tasks 1 and 2 evaluated with Cluster Web and OpsWeb}
    \label{cw_ow_times}
    \begin{tabular}{| l | l | l | l | l | l | l | }
    \hline
    Task durations (s) & 1     & 2     & 3      & 4      & Average & Standard Deviation \\
    \hline
    CW Task 1            & 60,64 & 54,48 & 108,77 & 65,88  & 72,44   & 24,66              \\
    CW Task 2            & 56,11 & 65,06 & 42,06  & 86,22  & 62,36   & 18,51              \\
    OW Task 1            & 32,34 & 21,86 & 19,53  & 121,58 & 48,83   & 48,82              \\
    OW Task 2            & 28,33 & 19,91 & 34,53  & 40,29  & 30,77   & 8,73              \\
    \hline
    \end{tabular}
    \end{center}
\end{table}

Whether or not the difference is significant can be found out with a two-sample T-test \cite{student1908probable, t_test} between the results of Cluster Web and OpsWeb. The null hypothesis is that there is no difference between Cluster Web and OpsWeb. In the test, a P-value is calculated. This value is the likelihood that the difference between two sample groups would be as large as it is if the null hypothesis were true. If the P-value is lower than the significance level 0.05, it can be said that there is a statistically significant difference at that significance level. The P-value used is for two-tails, meaning that OpsWeb could be either faster or slower than Cluster Web, and both possibilities are taken into account.

\begin{table}[!ht]
% Add some padding to the table cells:
\def\arraystretch{1.1}%
    \begin{center}
    \caption{T-test results between Cluster Web and OpsWeb task durations}
    \label{cw_ow_times_t_test}
    \begin{tabular}{| l | l | l | l | }
    \hline
    Test & Cluster Web sample size & OpsWeb sample size  & P-value   \\
    \hline
    Task 1 & 4 & 4 & 0,437 \\
    Task 2 & 4 & 4 &  0,037\\
    Tasks 1 and 2 & 8 & 8 & 0,073\\
    \hline
    \end{tabular}
    \end{center}
\end{table}

The results of the T-tests in table \ref{cw_ow_times_t_test} show that only task 2 has a P value that is below the treshold (0.037 < 0.05). It can be said that task 2 was performed significantly faster with OpsWeb than with Cluster Web.

Task 1 result can be considered statistically insignificant because it has a large P value of 0.437. The overall comparison of Cluster Web and OpsWeb with both tasks 1 and 2 is also above the 0.05 treshold with a P value of 0.073. This could be because of the small sample size, and the issues specified in section \ref{evaluation_problems} which caused some outliers in the data which raise the variance a lot. With a larger sample size, it may be possible to get better results.

The amount of mouse clicks appears to also show some difference between Cluster Web and OpsWeb. The averages for OpsWeb are seemingly lower than for Cluster Web \ref{cw_ow_clicks}. The signifigance of these results was again studied using a T-test.

\begin{table}[!ht]
% Add some padding to the table cells:
\def\arraystretch{1.1}%
    \begin{center}
    \caption{Amount of mouse clicks of tasks 1 and 2 evaluated with Cluster Web and OpsWeb}
    \label{cw_ow_clicks}
    \begin{tabular}{| l | l | l | l | l | l | l | }
    \hline
    Mouse clicks & 1  & 2  & 3  & 4  & Average & Standard Deviation \\
    \hline
    CW Task 1    & 14 & 9  & 21 & 28 & 18,00   & 8,29               \\
    CW Task 2    & 6  & 13 & 10 & 13 & 10,50   & 3,32               \\
    OW Task 1    & 5  & 4  & 4  & 4  & 4,25    & 0,50               \\
    OW Task 2    & 7  & 5  & 9  & 9  & 7,50    & 1,91            \\
    \hline
    \end{tabular}
    \end{center}
\end{table}

\begin{table}[!ht]
% Add some padding to the table cells:
\def\arraystretch{1.1}%
    \begin{center}
    \caption{T-test results between Cluster Web and OpsWeb mouse click amounts}
    \label{cw_ow_clicks_t_test}
    \begin{tabular}{| l | l | l | l | }
    \hline
    Test & Cluster Web sample size & OpsWeb sample size  & P-value   \\
    \hline
    Task 1 & 4 & 4 & 0,045 \\
    Task 2 & 4 & 4 & 0,178 \\
    Tasks 1 and 2 & 8 & 8 & 0,013 \\
    \hline
    \end{tabular}
    \end{center}
\end{table}

The T-test done on the mouse click amounts shows that task 1 by itself indicates a significant difference between Cluster Web and OpsWeb (0.045 < 0.05) while task 2 does not (0.178 > 0.05). When samples from task 1 and 2 are combined, the resulting P-value is 0.013 which means that overall it required significantly less mouse clicks to perform tasks with OpsWeb than with Cluster Web.

The mouse movement distance results show that with OpsWeb, task 1 was performed on average with shorter mouse movement distance than with Cluster Web, but with task 2 the result is opposite. T-test is done with the aim to find out if there is any significant difference between OpsWeb and Cluster Web in this regard.

\begin{table}[!ht]
% Add some padding to the table cells:
\def\arraystretch{1.1}%
    \begin{center}
    \caption{The mouse movement distance of tasks 1 and 2 evaluated with Cluster Web and OpsWeb}
    \label{cw_ow_distances}
    \begin{tabular}{| l | l | l | l | l | l | l | }
    \hline
    Mouse distance (m) & 1  & 2  & 3  & 4  & Average & Standard Deviation \\
    \hline
    CW Task 1          & 1,31 & 1,45 & 6,49 & 4,08 & 3,33    & 2,46               \\
    CW Task 2          & 1,78 & 2,14 & 0,99 & 2,04 & 1,74    & 0,52               \\
    OW Task 1          & 1,78 & 1,54 & 2,19 & 2,39 & 1,98    & 0,39               \\
    OW Task 2          & 2,58 & 1,44 & 6,74 & 4,18 & 3,74    & 2,30              \\
    \hline
    \end{tabular}
    \end{center}
\end{table}

\begin{table}[!ht]
% Add some padding to the table cells:
\def\arraystretch{1.1}%
    \begin{center}
    \caption{T-test results between Cluster Web and OpsWeb mouse movement distances}
    \label{cw_ow_distances_t_test}
    \begin{tabular}{| l | l | l | l | }
    \hline
    Test & Cluster Web sample size & OpsWeb sample size  & P-value   \\
    \hline
    Task 1 & 4 & 4 & 0,355 \\
    Task 2 & 4 & 4 & 0,188 \\
    Tasks 1 and 2 & 8 & 8 & 0,7315 \\
    \hline
    \end{tabular}
    \end{center}
\end{table}

As can be seen in table \ref{cw_ow_distances_t_test}, the P values for the mouse movement distance T-tests are rather large, so there isn't any significant difference between Cluster Web and OpsWeb mouse movement distance based on the data.

To sum up, OpsWeb shows shorter task duration times, but the results aren't statistically convincing enough, possibly due to small sample size and problems encountered during the evaluation. OpsWeb seems to require a lower amount of mouse clicks to perform a task than Cluster Web, and it appears to be a statistically significant difference. Mouse movement distances on the other hand do not show any kind of trend. More evaluation would be required to draw strong conclusions, but OpsWeb shows promise in making the user workflow faster and easier.

The error rates of the task answers are shown in table \ref{cw_ow_errors}. All answers were correct with Cluster Web, showing that it is a reliable tool and people know how to use it. With OpsWeb, users got task 1 wrong half of the time, which can be attributed to misunderstanding the task as defined in section \ref{evaluation_problems}. 

No participant got the task 2 answer right with OpsWeb. Author re-checked the correct answer multiple times, and a possible explanation to the high error rate is an usability issue with the calendar input that is explained in section \ref{usability_issues} .

\begin{table}[!ht]
% Add some padding to the table cells:
\def\arraystretch{1.1}%
    \begin{center}
    \caption{Error rates of tasks 1 and 2 evaluated with Cluster Web and OpsWeb}
    \label{cw_ow_errors}
    \begin{tabular}{| l | l | l | }
    \hline
    Task & Sample size & Error rate  \\
    \hline
    CW Task 1    &  4 & 0\%       \\
    CW Task 2    &  4 & 0\%       \\
    OW Task 1    &  4 & 50\%       \\
    OW Task 2    &  4 & 100\%       \\
    \hline
    \end{tabular}
    \end{center}
\end{table}

\subsection{Configuration comparison}
OpsWeb's default and Estrack configurations were evaluated to find out if using different configurations can make looking for the same information easier.

The configuration tasks didn't provide any conclusive data. In some cases Default configuration has better averages than Estrack configuration, in some cases worse, but there's no clear trend visible and all of the P-values are well above 0.05.

\subsection{User experience questionnaire}
The results of the UEQ revealed that the users had overall a more positive experience with OpsWeb than with Cluster Web. All different factors got higher averages on OpsWeb.

UEQ benchmark was used for analyzing the results. \cite{schrepp2017construction} The benchmark provides an Excel sheet where the results are inserted, and statistical calculations are done based on the data. The five factors: attractiveness, perspiquity, efficiency, dependability, stimulation and novelty are given a rating by the benchmark. Results for Cluster Web can be seen in figure \ref{fig:cw_ueq} and for OpsWeb in figure \ref{fig:ow_ueq}.

The rating system of the UEQ benchmark created by Schrepp et al. is based on a data set collected from studies which used UEQ. It consists of 246 studies with answers from 9905 persons. The benchmark intervals are as follows:
\begin{itemize}
    \item bad (75\% of the results in the data set are better)
    \item below average (50\% of the results are better, 25\% are worse)
    \item above average (25\% of the results are better, 50\% are worse)
    \item good (10\% of the results are better, 75\% worse)
    \item excellent (best 10\% of the results)
\end{itemize}\cite{schrepp2017construction}

On a numerical scale, the UEQ answers can be from 1 to 7, and in the benchmark they're shifted to be from -3 to 3 and the polarities are flipped when necessary so that -3 is in the negative end of the item pool and 3 in the positive end. What is notable is that on the benchmark ratings, for the most part anything below 1 is already in the "below average" category. On the answer scale that means below 5. This indicates that people usually give products quite high ratings with UEQ.

OpsWeb's results are in the "excellent" range of the benchmark for all factors, and for Cluster Web they are in the "bad" range. If we look at the averages, OpsWeb's are around 2 and Cluster Web's are around 0, on a scale from -3 to 3. 

"Efficiency" was the scale with largest difference between Cluster Web and OpsWeb. For Cluster Web this was the lowest scoring scale with a rating of -0.406, while for OpsWeb it was the highest scoring scale with a rating of 2.344. Efficiency covers how fast, organized and practical the system is considered to be.

Smallest difference between Cluster Web and OpsWeb was on the scale "Dependability", where the ratings for Cluster Web and OpsWeb were 0.563 and 1.844 respectively. For Cluster Web this was the highest scoring scale, while for OpsWeb it was the second lowest scoring. Dependability covers things like predictability, supportiveness, security and meeting expectations.

 %Pictures in .eps if you use latex, .pdf or .png if you use pdflatex. Don't specify the extension so you can use both!
\begin{figure}[ht]
  \begin{center}
    \includegraphics*[width=1\textwidth]{cw_ueq}
  \end{center}
  \caption{UEQ benchmark results for Cluster Web}
  \label{fig:cw_ueq}
\end{figure}

 %Pictures in .eps if you use latex, .pdf or .png if you use pdflatex. Don't specify the extension so you can use both!
\begin{figure}[ht]
  \begin{center}
    \includegraphics*[width=1\textwidth]{ow_ueq}
  \end{center}
  \caption{UEQ benchmark results for OpsWeb}
  \label{fig:ow_ueq}
\end{figure}

Comparison between UEQ results of Cluster Web and OpsWeb can be seen in figure \ref{fig:ueq_error_bars}. These figure also shows error bars representing a confidence interval at 95\% confidence level, which indicates how far away the true value could be from the reported value if a 5\% chance of error is considered to be significant. Because the error bars for Cluster Web and OpsWeb do not overlap, this would indicate that the difference between the two is significant. T-test with a signifigance level of 0.05 also shows a significant difference between Cluster Web and OpsWeb, as can be seen in table \ref{table:ueq_t_test}.

 %Pictures in .eps if you use latex, .pdf or .png if you use pdflatex. Don't specify the extension so you can use both!
\begin{figure}[ht]
  \begin{center}
    \includegraphics*[width=1\textwidth]{ueq_error_bars}
  \end{center}
  \caption{UEQ results for Cluster Web and OpsWeb with error bars. Cluster Web is in blue and OpsWeb is in red.}
  \label{fig:ueq_error_bars}
\end{figure}

\begin{table}[!ht]
% Add some padding to the table cells:
\def\arraystretch{1.1}%
    \begin{center}
    \caption{T-test results for UEQ scales between Cluster Web and OpsWeb}
    \label{table:ueq_t_test}
    \begin{tabular}{| l | l | }
    \hline
    Scale & P-value  \\
    \hline
Attractiveness & 0,0009  \\
Perspicuity    & 0,0141 \\
Efficiency     & 0,0001 \\
Dependability  & 0,0149 \\
Stimulation    & 0,0024 \\
Novelty        & 0,0004 \\
    \hline
    \end{tabular}
    \end{center}
\end{table}


There do not seem to be huge differences between different factors within Cluster Web and OpsWeb. Dependability scores highest for Cluster Web, and for OpsWeb the highest value is for efficiency.

UEQ benchmark also calculates values which can be used to analyze the trustworthiness of the evaluation. Participants might answer different items in an inconsistent way. For example, if a participant answers the item "clear - confusing" to be closer to "clear", and the item "not understandable - understandable" to be closer to "not understandable", it is possible that either the answers are not serious or the options were interpreted in a different way than intended. In the benchmark, having a difference of more than 3 points between two items which should match is considered to be an inconsistency.

According to the benchmark, having one inconsistent scale for a participant is not necessarily an issue, but if the same participant has two or more problematic scales this could be an indication of a problem. 

In OpsWeb's evaluation, one participant had more than one inconsistent scales: perspiquity and novelty in this case. For Cluster Web there were two participants who had more than one inconsistent scales. One of them had inconsistent answers for efficiency and dependability and the other for attractiveness, perspiquity, efficiency and dependability.

The benchmark calculates Cronbach's Alpha-coefficient \cite{cronbach1951coefficient} values for the scales to help determine how consistent the answers are. Items in the same scale should have a high correlation. 0.7 is commonly used as a treshold value, above which the answers can be considered to be consistent.

The alpha-coefficients can be seen in table \ref{alpha_coeff}. There are two scales which have a coefficient that is below 0.7: Cluster Web's novelty scale and OpsWeb's stimulation scale. They're not badly below 0.7 so this is not necessarily an indication of a big problem, but somewhat weakens the validity of those scales.

\begin{table}[!ht]
% Add some padding to the table cells:
\def\arraystretch{1.1}%
    \begin{center}
    \caption{Cronbach's Alpha-coefficients for Cluster Web's and OpsWeb's UEQ scales}
    \label{alpha_coeff}
    \begin{tabular}{| l | l | }
    \hline
    Scale & Alpha  \\
    \hline
    CW Attractiveness    & 0.81       \\
    CW Perspiquity    & 0.83        \\
    CW Efficiency    & 0.73        \\
    CW Dependability    & 0.73        \\
    CW Stimulation    & 0.87        \\
    CW Novelty    & 0.68        \\
    \hline
    OW Attractiveness    & 0.96        \\
    OW Perspiquity    & 0.74        \\
    OW Efficiency    & 0.72        \\
    OW Dependability    & 0.81        \\
    OW Stimulation    & 0.64        \\
    OW Novelty    & 0.76        \\
    \hline
    \end{tabular}
    \end{center}
\end{table}

\subsection{Effects of experience level on user experience and task performance}

The effect of participants' self-reported experience level (table \ref{experience_levels}) on the UEQ results were evaluated. Participants who answered 4 or more were considered to be experienced, while 4 or less were considered inexperienced.

Because only one participant reported to be inexperienced with Cluster Web, the results could not be statistically compared. That participant responded to Cluster Web more positively than the average, in particular on the scales "perspiquity" and "efficiency", which were above the error bars. The participant also took longer than average to complete a task with Cluster Web, but as this is a single data point, it cannot be said whether or not inexperienced users would generally perform tasks slower or not, even if it appears intuitive.

With OpsWeb, the comparison of UEQ results and task completion times between inexperienced and experienced users did not show any kind of a trend or statistically significant difference based on T-tests.

\subsection{Problems with the evaluation} \label{evaluation_problems}
Some problems with the evaluation itself were encountered due to possibly unclear presentation of the tasks. These problems could have effected OpsWeb´s evaluation results negatively.

Task 1 ("In the last 7 days of September this year, how many Maspalomas passes did Cluster 1 have?") was misunderstood on three out of four occasions when it was performed on OpsWeb. On two of those occasions, the participants counted passes for all spacecraft and not only Cluster 1, so they got the wrong answer. On one occasion, the participant started looking for passes during the last seven days and not the last seven days of September at first, which took some extra time.

On one occasion, task 2 ("In the last 7 days of September this year, how many Maspalomas passes did Cluster 1 have?") was misunderstood when evaluating OpsWeb and the participant started looking for passes during October at first. However, the task was restarted.

The Estrack configuration was not properly explained at first by the author on two occasions, so the configuration task for it took some extra time.

The questions were on paper and not always read out loud by the author. The misunderstood questions had a line break so it's possible that the second line was missed. 

The lesson learned from these problems is that every aspect of the evaluation should be pre-planned, like how the questions are presented to the user and how the system usage is explained during evaluation. It would also be good to have a pilot test of the evaluation to find potential issues before finalizing the plan and continuing on with the actual evaluation.

\subsection{Usability issues} \label{usability_issues}
One of the things discovered from the Cluster Web vs OpsWeb comparison tasks was that all of the task 2 answers were wrong for OpsWeb, as mentioned in section \ref{cw_ow_task_perf}. There is a potential usability issue that may have caused the participants to get the wrong answer. If the results are frequently wrong it lowers the effectiveness of the system.

When selecting the time range using the calendar input that is described in section \ref{opsweb_nav}, the user selects two dates which mark the beginning and end of the desired timeframe, and the schedule moves to show that timeframe. The problem is that the end date isn't actually shown; the selection is up to the beginning of the day. i.e. 0:00 UTC. This caused the participants to either miss the passes during the last day, or zoom out and look at a timeframe that is too long. The dates are still shown at the bottom axis of the schedule but it is possible that participants were not looking at it.

This would be easy to fix by changing the end date selection so that it selects from the beginning of the first day of the timeframe to the end of the last day, which could be more intuitive to the users.

Another thing that was discovered on one occasion is that the timeline labels aren't necessarily clear enough and easy to miss, so the user doesn't understand what data is shown on each timeline which is bad for learnability. Tweaking font size and colors could be considered.

\subsection{Usefulness of different configurations}
Participants answered to questions on how useful they found different configurations on a scale from 1 to 7. The answers can be seen in table \ref{config_usefulness}. Default configuration was found to be the most useful configuration by the users, unsurprisingly as it is the same as in Cluster Web. Estrack configuration was the second most useful configuration. Uberlog-specific configurations were found to be interesting but less useful. LEOP configuration was thought to be useful in certain scenarios, especially if other detailed information such as procedures and telemetry could be displayed for a single spacecraft.

\subsection{Usefulness of new features}
Participants rated how useful they found different new features of OpsWeb. Timeline navigation by mouse panning and zooming was rated highest along with the configurability of the timelines, evidently those two features are two of the biggest differences compared to Cluster Web and will hopefully make OpsWeb more usable. Features like Uberlog entries and activity countdown were found to be useful in certain scenarios; for example in a meeting it could be useful to visually view when Uberlog entries were made. Being able to change the color style of the schedule could be useful for example by using a dark theme during a night shift.

\begin{table}[!h]
% Add some padding to the table cells:
\def\arraystretch{1.1}%
    \begin{center}
    \caption{Participants' answers to questions about the usefulness of different configurations}
    \label{config_usefulness}
    \begin{tabular}{| l | l | l | l | l | l | l | l | l | l | }
    \hline
    Participant & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & Average  \\
    \hline
    Default    & 7  & 7 & 6 & 7 & 6 & 7 & 7 & 7 & 6.75   \\
    Estrack    & 7  & 7 & 6 & 7 & 5 & 5 & 7 & 7 & 6.375   \\
    Uberlog by severity    & 6  & 6 & 4 & 5 & 6 & 5 & 5 & 7 & 5.5   \\
    Uberlog by author    & 3  & 3 & 2 & 2 & 4 & 3 & 4 & 7 & 3.5   \\
    LEOP    & 6  & 6 & 7 & 6 & 5 & 6 & 4 & 7 & 5.875   \\
    \hline
    \end{tabular}
    \end{center}
\end{table}

\begin{table}[!h]
% Add some padding to the table cells:
\def\arraystretch{1.1}%
    \begin{center}
    \caption{Participants' answers to questions about the usefulness of new features}
    \label{features_usefulness}
    \begin{tabular}{| l | l | l | l | l | l | l | l | l | l | }
    \hline
    Participant & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & Average  \\
    \hline
    Mouse pan and zooming    & 7  & 7 & 7 & 7 & 6 & 7 & 7 & 7 & 6.875   \\
    Timeline configuration    & 7  & 7 & 7 & 7 & 6 & 7 & 7 & 7 & 6.875    \\
    Uberlog entries    &  6 & 7 & 5 & 6 & 5 & 6 & 5 & 7 & 5.875   \\
    Live mode    & 4  & 7 & 7 & 5 & 5 & 7 & 6 & 7 & 6   \\
    Activity countdown    & 6  & 6 & 6 & 5 & 5 & 6 & 6 & 7 & 5.875   \\
    Style configuration    & 4  & 6 & 7 & 7 & 5 & 7 & 6 & 7 & 6.125   \\
    Schedule URL sharing    & 6  & 7 & 7 & 7 & 6 & 6 & 6 & 7 & 6.5   \\
    \hline
    \end{tabular}
    \end{center}
\end{table}

\subsection{Ideas for future development}
Participants had many ideas for how OpsWeb could be improved in the future. The ability to plan passes and dynamically add other data on the timeline was mentioned, and this is indeed something that is planned for a future release.

An idea for increasing the clarity of the timeline was mentioned by a user: having some kind of a legend which would show the user what the different data types look like on the timeline. This could be dynamically generated based on the timeline configuration.

One participant wanted to have the ground station bookings from other missions filtered out when there is no visibility for Cluster. Cluster Web has this feature and it makes the view less cluttered, and it would be good to have it for OpsWeb as well.

Plotting telemetry data is a feature that one participant mentioned. OpsWeb can plot time series data on the timelines, and this could perhaps be further extended with axis labelling and perhaps a plot cursor that would allow the user to inspect the exact value at a given time.

Seeing loaded procedures and activities on the timeline would be useful according to one participant. As long as the data can be imported into the back-end this is something that could be implemented in the future.

One idea from a participant was that gaps in telemetry data could be displayed on a pass with different color so that the spacecraft controllers and engineers could see with a quick glance if there is a problem. Part of a pass could for example be colored in red rather than the usual blue color. Doing this in real time would require the back-end to keep track of the telemetry status. A concept of this idea can be seen in figure \ref{fig:concept}.

 %Pictures in .eps if you use latex, .pdf or .png if you use pdflatex. Don't specify the extension so you can use both!
\begin{figure}[ht]
  \begin{center}
    \includegraphics*[width=1\textwidth]{concept}
  \end{center}
  \caption{Displaying telemetry gaps on a pass}
  \label{fig:concept}
\end{figure}